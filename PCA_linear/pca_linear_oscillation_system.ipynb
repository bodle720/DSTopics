{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2353c52-ff45-4946-a306-b6116d7705c3",
   "metadata": {},
   "source": [
    "> PCA is a dimensionality reduction technique useful for gaining insight into the behavior of a data set. The resulting lower dimensional feature space can be used for further downstream ML tasks such as classification.\n",
    "\n",
    "There are multiple derivations of the results. The problem can be framed in equivalent ways, such as obtaining the eigenvector/eigenvalue pairs of the data's covariance matrix, the calulation of the Singular Value Decomposition (SVD), and solving an optimization problem. In this notebook, we briefly summarize the calulation of the Principal Components in a non-rigorous way, generate a contrived example from a system of differential equations, and see what inferences can be made from PCA in a purely data driven approach to analyszing that system.\n",
    "\n",
    "Before doing this, there are two important considerations prior to implementing PCA: _feature preprocessing_ and *assumptions of the algorithm*.\n",
    "\n",
    "<u>__Requirements__<u>\n",
    "\n",
    "- Data should be shifted to a mean of zero (subtract the mean of each feature).\n",
    "- Data should be recorded on the same scale. If they are different units or scales, then one may dominante the algorithm and return bad results. In this case the approach is often to standardize the data (subtract the mean and divide by the standard deviation).\n",
    "- There must be a linear correlation between the features. Otherwise, the covariance matrix is already daigonal and the features are independent. In this case, you can perform PCA by simply projecting onto the variables with the greates variance.\n",
    "\n",
    "<u>__Algorithm__<u>\n",
    "\n",
    "Assume observed data is organized in a matrix $X \\in R^{n \\times m}$, with $n$ samples of $m$ features each.\n",
    "\n",
    "PCA aims to find _p_ orthogonal directions on which to project the data, with $p<m$. These directions will capture the maximum variability in the data. In order to accomplish this, we calculate the covariance matrix of $X$:\n",
    "$$C \\equiv Cov(X) = \\frac{1}{n-1}X^TX \\in R^{m \\times m}$$\n",
    "This is a symmetric matrix with off diagonals non-zero. We can diagonalize $C$ by finding its corresponding eigevectors and eigenvalues:\n",
    "$$C = V \\Lambda V^{-1}$$\n",
    "\n",
    "Linear algebra can guarantee real eigenvalues and eigenvectors with the covariance matrix, though complex eigenvectors may exist.\n",
    "\n",
    "The entries in the diagonal matrix $\\Lambda = diag(\\lambda_0\\;\\;\\;\\lambda_1, ...)$ give the variances in each principal direction and the column vectors in $V$ give the principal directions we wish to project the data onto. The column entries in $V$ are called the *_loadings_ as they indicate how much of the original features are expressed in the new PCA features.\n",
    "\n",
    "*It should be noted here that _loadings_ and eigenvector entries are often considered separate concepts. Loadings are the eigenvectors (unit lenght) scaled up via multiplication by the eigenvalues. This equates to stretching the eigenvectors/principal axes/principal directions to reach the ends of the dataset. They are the same up to a constant scaling.\n",
    "\n",
    "If $PC_k$ represents the principal components projected down to $k$-dimensions, and $\\vec{v_i}$ is the $i$-th column vector of $V$, then:\n",
    "\n",
    "$$PC_k  = XV = X [\\vec{v_1}\\;\\;\\;\\vec{v_2}\\;\\;\\;...\\;\\;\\;\\vec{v_k}] = [X\\vec{v_1}\\;\\;\\;X\\vec{v_2}\\;\\;\\;...\\;\\;\\;X\\vec{v_k}] $$\n",
    "\n",
    "and $X\\vec{v_i}$ represents the linear combination of the columns of $X$ (i.e., the original $m$ features) with the entries in $\\vec{v_i}$ as coefficients. So, $\\vec{v_i}$ represents the 'importance' of each original feature is constructing the new feature space.\n",
    "\n",
    "This projection of our data is called the _principal components_ and is calculated as $Y = XV$, as above. The data in this new space can be shown to have a diagonal covariance matrix (by calculating $Y^{T}Y$). This means the data features are now independent in this eigenbasis.  __Connection__: If we were to standardize these new features in PC space by dividing by $\\sqrt{\\lambda_i}$, this would equate to performing whitening (or '_sphering_') on the data.\n",
    "\n",
    "<u>__SVD comments__<u>\n",
    "\n",
    "The SVD (Singular Value Decomposition) of X, written as the matrix factorization $$X = U \\Sigma V^T$$ can also yield the eigenvalues and eigenvectors. The eigenvectors will be the right singular (column) vectors ($V$) and the eigenvalues will be related to the singular values by the equation $$\\lambda_i = \\frac{\\sigma_{i}^2}{n-1}$$\n",
    "\n",
    "Futher, the principal components are calculated as $XV = U\\Sigma$ and the loadings are given by the columns of $\\frac{1}{\\sqrt{n-1}} V\\Sigma$.\n",
    "\n",
    "<u>__Application__<u>\n",
    "\n",
    "Consider the linear system of differential equations given by $A \\in R^{2 \\times 2}$: $$\\dot{\\vec{y}} = A \\vec{y}$$\n",
    "\n",
    "The solution $\\vec{y}(t) = [x_1(t), x_2(t)]^T$ for some initial condition at $t = 0$ has the form: $$\\vec{y} = e^{At}\\vec{y}(0)$$\n",
    "The matrix exponential evaluates to a matrix and can be calculated using the power series definition of $e^x$.\n",
    "\n",
    "The value of the matrix $A$ determines what type of system we have. We choose $A$ carefully so that it has purely imaginary (of the form $\\pm bi$ for some real b) eigenvalues. This guarantees the solution is a _center_ (elliptical solutions about the origin) and will not spiral to infinity or zero. Such a requirement is satisfied by setting: $$A = \\begin{bmatrix}0&1\\cr-9&0\\end{bmatrix}$$\n",
    "\n",
    "In this case, the eigenvalues are $\\pm 3i$. The corresponding eigenvectors are also complex because the eigenvalue is complex. They also come in conjugate pairs: $$\\vec{v} = \\begin{pmatrix} \\pm i /3 \\cr1\\cr\\end{pmatrix}$$\n",
    "\n",
    "Given the complex eigenvalues, we know this system has two linearly independent solutions that are real-valued. They will take the form\n",
    "$\\vec{s_1} = \\operatorname{Re}(s^*)$ and $\\vec{s_2} = \\operatorname{Im}(s^*) $, where $$s^* = \\begin{pmatrix} -i /3 \\cr1\\cr\\end{pmatrix} e^{3it} $$ \n",
    "\n",
    "The general solution is therefore given by: $$\\vec{y}(t) = c_1\\vec{s_1} + c_2\\vec{s_2}$$\n",
    "\n",
    "This gives us the general solution:\n",
    "\n",
    "$$\\vec{y}(t) = c_1 \\begin{pmatrix} sin(3t)/3 \\cr cos(3t) \\cr\\end{pmatrix} + c_2 \\begin{pmatrix} -cos(3t)/3 \\cr sin(3t) \\cr\\end{pmatrix}  $$\n",
    "\n",
    "If we start at the coordinate $\\begin{pmatrix} 3 \\cr3\\cr\\end{pmatrix} $, then our specific solution is parameterized by: $$ \\vec{y}(t) = \\begin{pmatrix} sin(3t) + 3cos(3t) \\cr 3cos(3t) -9sin(3t) \\cr\\end{pmatrix} $$\n",
    "\n",
    "This corresponds to an ellipse oscillating around the origin in the $x$ and $y$ directions. The following image shows the vector field of this system and the unique solution to our initial condition (gray square is the starting point, moving clockwise). The image is generated by the phase plane plotter [located here](https://aeb019.hosted.uark.edu/pplane.html) by Ariel Barton.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/oscillation_pca_med.png\" alt=\"Credit to Ariel Barton\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Note if $y = 0$, there is no motion is the x direction. Further, if $x = 0$, there is no motion in the $y$ direction. This is consistent with the $A$ matrix above.\n",
    "\n",
    "At this point, it is worth pointing out that the derivatives of each feature are related in a linear way by $A$, however the variables themselves are not linearly related. They are ellipses in $x$ and $y$ (quadratics). Generally, PCA is not a great approach for this type of problem as a system like this is often unstable and swirls to or from the origin. In this special case, however, the system is stable and is centered at the origin, so PCA might return _something_ valuable.\n",
    "\n",
    "The actual distribution for $x$ and $y$ will depend on the time dynamics of the problem. For example, it might turn the top and bottom corners more slowly than transversing the curve in the vertical direction. The quantity of data we see will depend on the interval of time we sample from and the sampling frequency (see below histograms). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fce6dc-25b2-490e-bf72-d8b1de2af26f",
   "metadata": {},
   "source": [
    "<u>__Truth Plots__<u>\n",
    "\n",
    "First, we take the solutions in time and plot them to better understand the system dynamics. Since one period of oscillation is $\\frac{2\\pi}{3}$ units of time, we plot from $t = 0$ to $t = p\\frac{2\\pi}{3} $ to visualize $p$ periods, coded below to be configurable. Note plotly allows you to hover over charts to view the values and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc73798c-4dd9-4296-8080-c4f9ef292b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b4bfee3-8fd6-4745-8f3b-22c66a6d0236",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = 5 # p >= 1\n",
    "L = p*(2*np.pi/3 + 0.01)\n",
    "t = np.arange(0, L , 0.1)\n",
    "x_t = np.sin(3*t) + 3*np.cos(3*t)\n",
    "y_t = 3*np.cos(3*t) - 9*np.sin(3*t)\n",
    "\n",
    "fig = px.line({'t': t, 'x(t)': x_t, 'y(t)':y_t}, x='t', y = ['x(t)', 'y(t)'], title = f'Solutions in time with {p} periods shown')\n",
    "for i in range(1, p + 1):\n",
    "    fig.add_vline(x = i*2*np.pi/3)\n",
    "#fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3e5c4cc-a250-4d9d-8abd-b498330d0376",
   "metadata": {},
   "source": [
    "Here is a static image produced from the above plot for viewing in GitHub. For an interactive plot and to experience the interactive capabilities of plotly, I recommnd viewing this notebook in a Jupyter environment.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/solutions_in_time_fig_1_medium.png\" alt=\"Static non-interactive image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6214a6f-a16e-47cd-80cb-61662158aa54",
   "metadata": {},
   "source": [
    "Now we visualize the distribution of $x$ and $y$ for a simulated trip around the ellipse by setting a specific sampling frequency $\\Delta t$ and $p$ periods of revolution. We can see the solution will have a distribution heavily weighted towards the ends for both $y$ and $x$ as 'corners' are turned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a5b7bc-3e46-4c61-bde5-9ab56d2bdfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "p = 3\n",
    "nbins = 100\n",
    "L = p*(2*np.pi/3)\n",
    "t = np.arange(0, L , dt)\n",
    "x_t = np.sin(3*t) + 3*np.cos(3*t)\n",
    "y_t = 3*np.cos(3*t) - 9*np.sin(3*t)\n",
    "fig = px.histogram({'t': t, 'x(t)': x_t, 'y(t)':y_t},\n",
    "                   x= ['x(t)', 'y(t)'],\n",
    "                   title = f'Histogram of x and y using dt = {dt} with {nbins} bins over {p} periods.',\n",
    "                   marginal=\"rug\",\n",
    "                   opacity=0.5,\n",
    "                   barmode = 'overlay',\n",
    "                   nbins = nbins)\n",
    "#fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea106564-ede9-4212-9c87-a7bbe0c89e91",
   "metadata": {},
   "source": [
    "Similarly, here is a static rendering of the above plot without the benefits of interactivity.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/hist_of_x_and_y_fig_2_medium.png\" alt=\"Static non-interactive image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e761da-b856-4c4e-b780-215dc45fe1f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<u>__Data Driven Analysis__<u>\n",
    "\n",
    "Our goal now is to start with this solution and simulate observed data points from this system using multiple viewpoints in $3D$ space. This will provide a high dimensional observation set of what should be a $2D$ system. We introduce random noise to simulate error in the sensors or the system. Then, we will run a Principal Component Analysis and see what information it reveals. Note the output of PCA will vary based on the composition of this dataset. For example, the long and narrow features of the starting solution might be lost. We will choose viewpoints that capture the spatial relationships of $x$ and $y$ well with minmial loss of information. As an alternative, one could perform PCA on the initial dataset itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27856805-3f1d-42a9-aadc-749bdd603568",
   "metadata": {},
   "source": [
    "First, add noise to the system and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a11c739-a82a-4e06-aaa1-9fd8b3647f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 30*(2*np.pi/3) #do 20 rotations of the ellipse\n",
    "t = np.arange(0, L , 0.1)\n",
    "x_t = np.sin(3*t) + 3*np.cos(3*t) + np.random.normal(loc=0.0, scale=0.6, size=len(t))\n",
    "y_t = 3*np.cos(3*t) - 9*np.sin(3*t) + np.random.normal(loc=0.0, scale=0.6, size=len(t))\n",
    "fig = px.scatter({'t': t, 'x(t)': x_t, 'y(t)':y_t},\n",
    "                 x='x(t)',\n",
    "                 y = 'y(t)',\n",
    "                 title = f'x versus y scatter plot with normal noise',\n",
    "                 width = 500,\n",
    "                 height = 700,\n",
    "                 range_x = [-6, 6],\n",
    "                 range_y = [-12, 12])\n",
    "#fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c16d58a-7cfd-4d47-b68c-1b6e329b6e2c",
   "metadata": {},
   "source": [
    "Static result:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/scatter_of_x_and_y_fig_3_medium.png\" alt=\"Static non-interactive image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8316b-3105-4857-92c6-7f3bbbd47b4d",
   "metadata": {},
   "source": [
    "We now define four planes using their orthogonal basis vectors and project the data onto them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a1187b-67ac-428e-a765-5fb8c335bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_points = np.vstack([x_t, np.zeros(len(x_t)), y_t])\n",
    "\n",
    "#Play with this variable to see how end result changes.\n",
    "#The following will slightly rotate our viewpoint so that \n",
    "#the end results will be roughly the shape we started with,\n",
    "#but this wont always be the case.\n",
    "planes_rotation = [[[1, 0, 0],[0, 0, 1]], \n",
    "                      [[2, 2, 0],[0, 0, 1]],\n",
    "                      [[3, 5, 0],[0, 0, 1]], \n",
    "                      [[5, 7, 0],[0, 0, 1]]]\n",
    "\n",
    "#planes_rotation = [[[1, 0, 0],[0, 0, 1]],\n",
    "#                   [[1, 0, 0],[0, 0, 1]]]\n",
    "\n",
    "proj_arrs_categ = {}\n",
    "for ix, plane in enumerate(planes_rotation):\n",
    "    # make unit vectors, this helps make sure the projections are each referring to the same 'unit' length\n",
    "    plane_arr = np.array(plane).T\n",
    "    normed_basis = plane_arr/np.linalg.norm(plane_arr, axis = 0)\n",
    "    #print(normed_basis.T @ normed_basis) #should be the identity\n",
    "    # project the data, only need the transpose\n",
    "    proj = normed_basis.T @ initial_points\n",
    "    proj_arrs_categ[ix] = proj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e322603-e3bd-4946-8d73-53cc4652e01b",
   "metadata": {},
   "source": [
    "Now, we plot the new data to inspect it. Note the above vectors correspond to rotating your viewpoint about the vertical axis if viewing the orginal data in 3 dimensions with $y$-coordinates of $0$ on the xyz-coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d2d025-a36c-46f1-a6e6-9a17d9d5de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = []\n",
    "for i in range(len(proj_arrs_categ)):\n",
    "    pts = proj_arrs_categ[i] #shape (2, N)\n",
    "    fig = px.scatter(x=pts[0,:],\n",
    "                 y = pts[1,:],\n",
    "                 title = f'Data projected onto plane {i + 1}',\n",
    "                    range_x = [-5, 5],\n",
    "                    range_y = [-10,10])\n",
    "    figures.append(fig)\n",
    "\n",
    "fig = make_subplots(rows=len(proj_arrs_categ), cols=1, shared_xaxes=True) \n",
    "for i, figure in enumerate(figures):\n",
    "    for trace in range(len(figure[\"data\"])):\n",
    "        fig.add_trace(figure[\"data\"][trace], row = i+1, col = 1)\n",
    "        \n",
    "#fig.update_layout(height=600, width=300, title_text=\"New measured data.\")\n",
    "#fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "caf9c33b-63a9-4391-ac20-900321aaf61e",
   "metadata": {},
   "source": [
    "Static result:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/new_data_fig_4_medium.png\" alt=\"Static non-interactive image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4969ef5-a049-47bb-ab33-975be4779f3d",
   "metadata": {},
   "source": [
    "Form the full projected dataset acting as the observd data. It has shape (N samples, 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef9f984d-6c43-4627-89ac-66dd554c3f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(629, 8)\n"
     ]
    }
   ],
   "source": [
    "arr_ls = [i.T for i in proj_arrs_categ.values()]\n",
    "X = np.hstack(arr_ls)\n",
    "print(X.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e030d-854d-4b7b-a393-9f574fae4b70",
   "metadata": {},
   "source": [
    "Perform the steps of PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "012b1c39-6ca7-4a8d-8eec-ea3f1b576929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues =  (180.5383407316052, 11.375359938936901, 4.500478557596392e-16, 1.6351381379258332e-31, -1.4284327653024453e-30)\n",
      "Explained variance =  100.0 %\n"
     ]
    }
   ],
   "source": [
    "X_mean_sub = X - np.mean(X, axis = 0)\n",
    "\n",
    "n = len(X_mean_sub)\n",
    "C = (1/(n-1))*X_mean_sub.T @ X_mean_sub\n",
    "eigenvalues, eigenvectors = scipy.sparse.linalg.eigs(C, min(5, X_mean_sub.shape[1] - 2), which = 'LR')\n",
    "eigenvalues = [i.real for i in eigenvalues]\n",
    "eigenvectors = [eigenvectors[:,i].real for i in range(eigenvectors.shape[1])]\n",
    "\n",
    "sorted_evals, sorted_evecs = zip(*sorted(zip(eigenvalues, eigenvectors), reverse = True))\n",
    "\n",
    "#plot the eigenvalues\n",
    "fig = px.scatter(x=range(1, len(sorted_evals) + 1),\n",
    "                 y = sorted_evals,\n",
    "                 title = f'Eigenvalues/variances')\n",
    "#fig.show()\n",
    "\n",
    "#find the principal components using the first two eigenvectors\n",
    "V = np.hstack([sorted_evecs[0].reshape(-1, 1), sorted_evecs[1].reshape(-1, 1)])\n",
    "PC = X_mean_sub @ V #shape (N, 2)\n",
    "explained_variance = sum(sorted_evals[:2])/sum(sorted_evals)\n",
    "print('Eigenvalues = ', sorted_evals[:5])\n",
    "print('Explained variance = ', 100*explained_variance, '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "064aed43-186c-4530-8519-0baa340c12a3",
   "metadata": {},
   "source": [
    "Static result:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/eigenvalues_fig_5_medium.png\" alt=\"Static non-interactive image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0e335a-4676-48be-a715-773fc14feac7",
   "metadata": {},
   "source": [
    "It is clear the vast majority of the variance in this system is captured by the <u>first two principal components</u>, so we projected onto the first two eigenvectors. We started with 8-dimensional data and were able to reduce it down to 2 dimensions while preserving nearly $100\\%$ of the variance. This is a very synthetic example, so in reality expect a much lower percentage.\n",
    "\n",
    "Now, let's see what $x$ and $y$ look like when graphed together as a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af54c792-b1bf-43bc-a9dc-8c82ff0ded6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=PC[:,0],\n",
    "                 y = PC[:,1],\n",
    "                 title = f'Data projected onto the first two Principal Directions')\n",
    "#fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5210c516-6cb1-46b3-a557-cdf6a56dfdba",
   "metadata": {},
   "source": [
    "Static image:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/data_projection_fig_6_medium.png\" alt=\"Static non-interactive image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b000a-e070-4524-8877-b49bf2603141",
   "metadata": {},
   "source": [
    "We have what roughly looks like our original ellipse, but rotated and scaled. We can still detect the fact that the boundaries of the range of the major axis are approximately $3$ times that of the minor axis. We can see from the truth $x(t)$ and $y(t)$ plots from the beginning of the notebook that when $x = \\pm3$, $y$ is increasing quickly to $\\pm 9$ and stays there until the slower $x$ dynamics push it over the corner to the next corner. This is shown by the clustering of points on the ends of the ellipse.\n",
    "\n",
    "We now view the time dynamics that the Principal Components estimated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "845f3ad6-3a32-49b9-ba72-c29f73e57f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = np.arange(len(PC))\n",
    "\n",
    "fig = px.line({'t': t, 'x_pc(t)':PC[:,0], 'y_pc(t)': PC[:,1]}, x = 't', y = ['x_pc(t)', 'y_pc(t)'], title = f'PC output over time')\n",
    "#fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56050be3-a5d4-4903-ace8-c64c1c7a15e0",
   "metadata": {},
   "source": [
    "Static image:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/pc_output_fig_7_medium.png\" alt=\"Static non-interactive image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5979057-a51b-4776-9743-d02e91d369fd",
   "metadata": {},
   "source": [
    "This is a pretty close approximation to the above $x(t)$ and $y(t)$ plots, although scaled.\n",
    "\n",
    "I hope this notebook was interesting/educational. Thank you for reading."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
