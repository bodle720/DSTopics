{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56b775e-3df5-4231-ad2c-c2018c939942",
   "metadata": {},
   "source": [
    "## Robust Principal Component Analysis (RPCA)\n",
    "\n",
    "> Robust PCA is a technique for decomposing a data matrix into two terms: a low-rank matrix capturing the characteristics of interest and a sparse matrix containing noise that captures corrupted the data.\n",
    "\n",
    "Standard PCA can be thought of as a maximization problem (maximizing the preserved variance of the data) or a minimization problem (minimizing the squared errors between data and their projection on the principal axes). Using this approach, the solution will be strongly skewed towards the outliers as they use an $L2$-norm, making the solution sensitive to outliers (not robust). That is to say, PCA is a fragile approach that will fail when your data is either corrupted or incomplete. An answer to this dilemma was developed by Candes, Li, Ma, and Wright in their 2009 paper _\"Robust Principal Component Analysis?\"_ (see __[1]__ in __References__).\n",
    "\n",
    "Like Sparse PCA (see my [notebook on Sparse PCA](../sparsePCA/sparsePCA.ipynb)), there are multiple approaches to solving this problem. This notebook will only set up the problem and refer the reader to specifics on theory and derivation.\n",
    "\n",
    "Given a data matrix $X \\in R^{n \\times m}$, we wish to decompose it as the sum of a low-rank matrix $L$ and a sparse matrix $S$: $$X = L+S$$\n",
    "\n",
    "Here, $X$ contains our corrupted data, $L$ is of low rank and contains our corrected data, and $S$ will hold all the outlier measurements. Given this decomposition, $L$ will be a new dataset we can work with that will be well approximated by standard PCA or other machine learning algorithms. Mathematically, this (idealized) problem can be written as $$\\underset{L, S}{min}\\;\\;\\{rank(L)+||S||_0\\}\\;\\;s.t.\\;\\;X = L+S$$\n",
    "\n",
    "This problem is intractable as neither component is convex and there is no guarantee to a solution. To solve this, a convex relaxation is introduced using proxies for the actual operators of interest. The rank is replaced by the _nuclear norm_ (the sum of singular values - more zero singular values means lower rank) and the $0$-norm is replaced by the $1$-norm (sum of absolute values).\n",
    "\n",
    "$$\\underset{L, S}{min}\\;\\;\\{||L||_{*}+\\lambda||S||_1\\}\\;\\;s.t.\\;\\;X = L+S$$\n",
    "\n",
    "\n",
    "The above formulation's solution converges to the idealized formulation with high probability providing $\\lambda =  \\frac{1}{\\sqrt{max(n,m)}}$ (see __[2]__).\n",
    "\n",
    "From this point, the method of Augmented Lagrange Multipliers can be applied (__[3]__) to find the desired decomposition. Per __[2]__, you can also utilize the Alternating Directions Method (ADM) by applying a shrinkage operator and a Singular Value Threshold (SVT) operator iteratively. The following code block defining the functions `shrink`, `SVT`, and `RPCA` is provided by __[2]__ and all credit goes to Steven L. Brunton, J. Nathan Kutz, and Daniel Dylewsky (for translating the original MATLAB code to Python) for these functions. We apply the results from the aformentioned functions to a problem involving denoising handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "127b6076-4c3b-4c3d-9bca-0c68487fcd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPCA from Steven L. Brunton; J. Nathan Kutz (2019)\n",
    "# Available at https://databookuw.com/\n",
    "import numpy as np\n",
    "\n",
    "def shrink(X,tau):\n",
    "    Y = np.abs(X)-tau\n",
    "    return np.sign(X) * np.maximum(Y,np.zeros_like(Y))\n",
    "    \n",
    "def SVT(X,tau):\n",
    "    U,S,VT = np.linalg.svd(X,full_matrices=0)\n",
    "    out = U @ np.diag(shrink(S,tau)) @ VT\n",
    "    return out\n",
    "    \n",
    "def RPCA(X):\n",
    "    n1,n2 = X.shape\n",
    "    mu = n1*n2/(4*np.sum(np.abs(X.reshape(-1))))\n",
    "    lambd = 1/np.sqrt(np.maximum(n1,n2))\n",
    "    thresh = 10**(-7) * np.linalg.norm(X)\n",
    "    \n",
    "    S = np.zeros_like(X)\n",
    "    Y = np.zeros_like(X)\n",
    "    L = np.zeros_like(X)\n",
    "    count = 0\n",
    "    while (np.linalg.norm(X-L-S) > thresh) and (count < 1000):\n",
    "        L = SVT(X-S+(1/mu)*Y,1/mu)\n",
    "        S = shrink(X-L+(1/mu)*Y,lambd/mu)\n",
    "        Y = Y + mu*(X-L-S)\n",
    "        count += 1\n",
    "    return L,S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86873d5b-2ffd-4a2a-8172-aedcba177dc8",
   "metadata": {},
   "source": [
    "__Application__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efca750-9e17-4245-b255-9d247650790e",
   "metadata": {},
   "source": [
    "<u>__References__<u>\n",
    "\n",
    "__[1]__ Emmanuel J. Candes; Xiaodong Li; Yi Ma; John Wright (2009). \"Robust Principal Component Analysis?\". Journal of the ACM. 58 (3): 1–37\n",
    "\n",
    "__[2]__ Steven L. Brunton; J. Nathan Kutz (2019). \"Data Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control\". Section 3.7 \n",
    "\n",
    "__[3]__ Hestenes, M. R. (1969). \"Multiplier and gradient methods\". Journal of Optimization Theory and Applications. 4 (5): 303–320."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
